global_ranges: # these filter down the entire dataset (global filter)
  n_nodes:
    min: 1  # Minimum size of graphs to use for training
    max: 1000 # Maximum size of graphs to use for training
  density:
    min: 0
    max: 1

ordering_ranges: # design specific feature combinations and ordering
  a_1:
    num_samples: 200
    rules:
      n_nodes:
        quantiles: [0.0, 0.25]
  a_2:
    num_samples: 200
    rules:
      n_nodes:
        quantiles: [ 0.25, 0.5 ]
  a_3:
    num_samples: 200
    rules:
      n_nodes:
        quantiles: [ 0.5, 0.75 ]
  a_4:
    num_samples: 200
    rules:
      n_nodes:
        quantiles: [ 0.75, 1.0 ]
  density_1:
    num_samples: 200
    rules:
      density:
        quantiles: [0.0, 0.25]
  density_2:
    num_samples: 200
    rules:
      density:
        quantiles: [0.25, 0.5]
  density_3:
    num_samples: 200
    rules:
      density:
        quantiles: [0.5, 0.75]
  density_4:
    num_samples: 200
    rules:
      density:
        quantiles: [0.75, 1.0]

shuffle: False # shuffling does not impact validation since we are running inference
seed: 1234
num_samples_per_category_default: 200
epochs: 1 # only need one pass through the data
drop_redundant: True # for validation, don't want any duplicate examples as this is wasteful

